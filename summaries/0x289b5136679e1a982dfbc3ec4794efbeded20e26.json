{
    "summary": "Deconstructing Bostrom\u2019s Argument for AI Doom - I had a pretty great discussion with social psychologist and philosopher Lance Bush recently about the orthogonality thesis, which ended up turning into a broader analysis of Nick Bostrom\u2019s argument for AI doom as presented in Superintelligence, and some related issues.\n\n Counting arguments provide no evidence for AI doom - This is Part 2 of an essay series that started with AI is easy to control. Introduction AI doom scenarios often suppose that future AIs will engage in scheming\u2014 planning to escape, gain power, and pursue ulterior motives, while deceiving us into thinking they are aligned with our interests. The worry is that if a\u2026\n\n AI is easy to control - Why are billions of dollars being poured into artificial intelligence R&D this year? Companies certainly expect to get a return on their investment. Arguably, the main reason AI is profitable is that it is more controllable than human labor. The personality and conduct of an AI can be controlled to much finer grained precision than\u2026\n\n Introducing AI Optimism - Artificial intelligence promises to greatly improve the quality of life of every human on Earth. Already, AI assistants are democratizing access to education, high-quality medical advice, and psychotherapy. Text-to-image models like Midjourney and Stable Diffusion have unleashed the creativity of the masses, empowering people to create stunning artwork at little or no cost. Tools like\u2026\n\n AI Pause Will Likely Backfire - Should we lobby governments to impose a moratorium on AI research? Since we don\u2019t enforce pauses on most new technologies, I hope the reader will grant that the burden of proof is on those who advocate for such a moratorium. We should only advocate for such heavy-handed government action if it\u2019s clear that the benefits\u2026\n\n Evolution provides no evidence for the sharp left turn - (Originally posted on LessWrong here) Does human evolution imply a sharp left turn from AIs? Arguments for the sharp left turn in AI capabilities often appeal to an \u201cevolution -> human capabilities\u201d analogy and say that evolution\u2019s outer optimization process built a much faster human inner optimization process whose capability gains vastly outstripped those which evolution built\u2026\n\n My Objections to \u201cWe\u2019re All Gonna Die with Eliezer Yudkowsky\u201d - Introduction I recently watched Eliezer Yudkowsky\u2019s appearance on the Bankless podcast, where he argued that AI was nigh-certain to end humanity. Since the podcast, some commentators have offered pushback against the doom conclusion. However, one sentiment I saw was that optimists tended not to engage with the specific arguments pessimists like Yudkowsky offered. Economist Robin Hanson points out that this pattern is very common for small groups which\u2026\n\n Deceptive Alignment is <1% Likely by Default - By David W. Thanks to Wil Perkins, Grant Fleming, Thomas Larsen, Declan Nishiyama, and Frank McBride for feedback on this post. Thanks also to Paul Christiano, Daniel Kokotajlo, and Aaron Scher for comments on the original post that helped clarify the argument. Any mistakes are my own. Introduction In this post, I argue that deceptive alignment is\u2026\n\n"
}